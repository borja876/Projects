{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import scipy\n",
    "import spacy\n",
    "import logging\n",
    "import sys\n",
    "import string\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import mode\n",
    "from time import time\n",
    "from string import punctuation\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPooling1D\n",
    "from keras.layers import LSTM, Input, TimeDistributed\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this exercise is to classify authors using different novels that they have written. In this case, five different implementations of neural networks using Tensorflow and Keras have been used and tuned. Results are compared between them in terms of accuracy and complexity of each model.\n",
    "\n",
    "Regarding the corpus the dataset has benn picked up randomly from the Gutenberg project and it is the same copus used in the unsupervides capstone project presented earlier. This project shares the initial cleasing stage with the capstone project. The authors that have been picked are:\n",
    "\n",
    "1. Jane Austen\n",
    "2. Chesterton\n",
    "3. Conan Doyle\n",
    "4. Charles Dickens\n",
    "5. Elliot\n",
    "6. Huxley\n",
    "7. Shakespeare\n",
    "8. Shaw\n",
    "9. H.G. Wells\n",
    "10. Oscar Wilde\n",
    "\n",
    "In this notebook we will see the following steps:\n",
    "\n",
    "1. Retreive and store the data creating the dataset\n",
    "2. Cleanse and parse and tokenize texts\n",
    "3. Multi Layer Perceptron\n",
    "4. Simple LTSM\n",
    "5. LTSM with Dropout\n",
    "6. LTSM & Convolutional Neural Network\n",
    "7. LTSM & Recurrent Neural Network\n",
    "8. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Retreive and store data creating the dataset ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seven novels from ten different authors have been retreived form Gutenberg project and a list of all the book files is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all of our book files.\n",
    "book_filenames_austen = sorted(glob.glob(\"/home/borjaregueral/train/austen/*.txt\"))\n",
    "book_filenames_chesterton = sorted(glob.glob(\"/home/borjaregueral/train/chesterton/*.txt\"))\n",
    "book_filenames_conandoyle = sorted(glob.glob(\"/home/borjaregueral/train/conandoyle/*.txt\"))\n",
    "book_filenames_dickens = sorted(glob.glob(\"/home/borjaregueral/train/dickens/*.txt\"))\n",
    "book_filenames_elliot = sorted(glob.glob(\"/home/borjaregueral/train/elliot/*.txt\"))\n",
    "book_filenames_huxley = sorted(glob.glob(\"/home/borjaregueral/train/huxley/*.txt\"))\n",
    "book_filenames_shakespeare = sorted(glob.glob(\"/home/borjaregueral/train/shakespeare/*.txt\"))\n",
    "book_filenames_shaw = sorted(glob.glob(\"/home/borjaregueral/train/shaw/*.txt\"))\n",
    "book_filenames_wells = sorted(glob.glob(\"/home/borjaregueral/train/wells/*.txt\"))\n",
    "book_filenames_wilde = sorted(glob.glob(\"/home/borjaregueral/train/wilde/*.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information is added to the copus and stored as raw books so that they can be cleansed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading '/home/borjaregueral/train/austen/austen-emma.txt'...\n",
      "Corpus is now 795775 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/austen/austen-loveandfriendship.txt'...\n",
      "Corpus is now 980773 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/austen/austen-mansfield.txt'...\n",
      "Corpus is now 1549557 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/austen/austen-northanger.txt'...\n",
      "Corpus is now 1990819 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/austen/austen-persuasion.txt'...\n",
      "Corpus is now 2457111 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/austen/austen-pride_and_prejudice.txt'...\n",
      "Corpus is now 3167694 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/chesterton/chesterton-allthingsconsidered.txt'...\n",
      "Corpus is now 337803 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/chesterton/chesterton-ballad.txt'...\n",
      "Corpus is now 452044 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/chesterton/chesterton-brown.txt'...\n",
      "Corpus is now 858673 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/chesterton/chesterton-heretics.txt'...\n",
      "Corpus is now 1206602 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/chesterton/chesterton-innocence.txt'...\n",
      "Corpus is now 1655080 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/chesterton/chesterton-manknewtoomuch.txt'...\n",
      "Corpus is now 1988969 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/conandoyle/conandoyle-lostworld.txt'...\n",
      "Corpus is now 424038 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/conandoyle/conandoyle-returnofsh.txt'...\n",
      "Corpus is now 1033223 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/conandoyle/conandoyle-signofthefour.txt'...\n",
      "Corpus is now 1266584 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/conandoyle/conandoyle-studyinscarlet.txt'...\n",
      "Corpus is now 1509528 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/conandoyle/conandoyle-talesofterror.txt'...\n",
      "Corpus is now 1929528 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/conandoyle/conandoyle-whitecompany.txt'...\n",
      "Corpus is now 2755298 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/dickens/dickens-childrenstories.txt'...\n",
      "Corpus is now 102958 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/dickens/dickens-christmascarol.txt'...\n",
      "Corpus is now 268260 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/dickens/dickens-cricketontheearth.txt'...\n",
      "Corpus is now 448410 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/dickens/dickens-greatexpectations.txt'...\n",
      "Corpus is now 1024911 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/dickens/dickens-messagefromthesea.txt'...\n",
      "Corpus is now 1093547 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/dickens/dickens-mutualfriend.txt'...\n",
      "Corpus is now 2192797 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/elliot/elliot-adambede.txt'...\n",
      "Corpus is now 1195716 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/elliot/elliot-deronda.txt'...\n",
      "Corpus is now 1999374 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/elliot/elliot-felixholt.txt'...\n",
      "Corpus is now 3020234 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/elliot/elliot-impressions.txt'...\n",
      "Corpus is now 3362988 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/elliot/elliot-liftedveil.txt'...\n",
      "Corpus is now 3462331 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/elliot/elliot-middlemarch.txt'...\n",
      "Corpus is now 4440095 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/huxley/huxley-autobiography.txt'...\n",
      "Corpus is now 320045 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/huxley/huxley-evolutionethics.txt'...\n",
      "Corpus is now 905999 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/huxley/huxley-gladstone.txt'...\n",
      "Corpus is now 961070 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/huxley/huxley-hume.txt'...\n",
      "Corpus is now 1344031 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/huxley/huxley-methodofzadig.txt'...\n",
      "Corpus is now 1377574 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/huxley/huxley-originofspecies.txt'...\n",
      "Corpus is now 1461388 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shakespeare/shakespeare-henryv.txt'...\n",
      "Corpus is now 154106 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shakespeare/shakespeare-macbeth.txt'...\n",
      "Corpus is now 254457 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shakespeare/shakespeare-merchantofvenice.txt'...\n",
      "Corpus is now 372013 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shakespeare/shakespeare-midsummernightsdream.txt'...\n",
      "Corpus is now 468263 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shakespeare/shakespeare-othello.txt'...\n",
      "Corpus is now 619192 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shakespeare/shakespeare-richard3.txt'...\n",
      "Corpus is now 810913 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shaw/shaw-barbara.txt'...\n",
      "Corpus is now 187464 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shaw/shaw-candida.txt'...\n",
      "Corpus is now 317306 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shaw/shaw-doctors.txt'...\n",
      "Corpus is now 498365 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shaw/shaw-heartbreak.txt'...\n",
      "Corpus is now 766405 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shaw/shaw-mandandsuperman.txt'...\n",
      "Corpus is now 1150457 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/shaw/shaw-married.txt'...\n",
      "Corpus is now 1485657 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/wells/wells-firstmeninthemoon.txt'...\n",
      "Corpus is now 386680 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/wells/wells-futureamerica.txt'...\n",
      "Corpus is now 752816 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/wells/wells-invisibleman.txt'...\n",
      "Corpus is now 1025166 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/wells/wells-island.txt'...\n",
      "Corpus is now 1267068 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/wells/wells-timemachine.txt'...\n",
      "Corpus is now 1446291 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/wilde/wilde-canterville.txt'...\n",
      "Corpus is now 64302 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/wilde/wilde-dorian.txt'...\n",
      "Corpus is now 497992 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/wilde/wilde-goodwoman.txt'...\n",
      "Corpus is now 611499 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/wilde/wilde-husband.txt'...\n",
      "Corpus is now 788834 characters long\n",
      "\n",
      "Reading '/home/borjaregueral/train/wilde/wilde-intentions.txt'...\n",
      "Corpus is now 1121413 characters long\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Read and add the text of each book to corpus_raw.\n",
    "corpus_raw_austen = u\"\"\n",
    "for book_filename in book_filenames_austen:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw_austen += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw_austen)))\n",
    "    print()\n",
    "    \n",
    "#Read and add the text of each book to corpus_raw.\n",
    "corpus_raw_chesterton = u\"\"\n",
    "for book_filename in book_filenames_chesterton:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw_chesterton += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw_chesterton)))\n",
    "    print()\n",
    "    \n",
    "#Read and add the text of each book to corpus_raw.\n",
    "corpus_raw_conandoyle = u\"\"\n",
    "for book_filename in book_filenames_conandoyle:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw_conandoyle += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw_conandoyle)))\n",
    "    print()\n",
    "\n",
    "#Read and add the text of each book to corpus_raw.\n",
    "corpus_raw_dickens = u\"\"\n",
    "for book_filename in book_filenames_dickens:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw_dickens += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw_dickens)))\n",
    "    print()\n",
    "\n",
    "#Read and add the text of each book to corpus_raw.\n",
    "corpus_raw_elliot = u\"\"\n",
    "for book_filename in book_filenames_elliot:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw_elliot += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw_elliot)))\n",
    "    print()    \n",
    "    \n",
    "#Read and add the text of each book to corpus_raw.\n",
    "corpus_raw_huxley = u\"\"\n",
    "for book_filename in book_filenames_huxley:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw_huxley += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw_huxley)))\n",
    "    print()\n",
    "\n",
    "#Read and add the text of each book to corpus_raw.\n",
    "corpus_raw_shakespeare = u\"\"\n",
    "for book_filename in book_filenames_shakespeare:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw_shakespeare += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw_shakespeare)))\n",
    "    print()\n",
    "    \n",
    "#Read and add the text of each book to corpus_raw.\n",
    "corpus_raw_shaw = u\"\"\n",
    "for book_filename in book_filenames_shaw:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw_shaw += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw_shaw)))\n",
    "    print()\n",
    "    \n",
    "#Read and add the text of each book to corpus_raw.\n",
    "corpus_raw_wells = u\"\"\n",
    "for book_filename in book_filenames_wells:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw_wells += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw_wells)))\n",
    "    print()\n",
    "    \n",
    "#Read and add the text of each book to corpus_raw.\n",
    "corpus_raw_wilde = u\"\"\n",
    "for book_filename in book_filenames_wilde:\n",
    "    print(\"Reading '{0}'...\".format(book_filename))\n",
    "    with codecs.open(book_filename, \"r\", \"utf-8\") as book_file:\n",
    "        corpus_raw_wilde += book_file.read()\n",
    "    print(\"Corpus is now {0} characters long\".format(len(corpus_raw_wilde)))\n",
    "    print()\n",
    "    \n",
    "#Create a list with all the corpus   \n",
    "doc_complete = [corpus_raw_austen, corpus_raw_chesterton,\n",
    "                corpus_raw_conandoyle, corpus_raw_dickens,\n",
    "                corpus_raw_elliot, corpus_raw_huxley,\n",
    "                corpus_raw_shakespeare, corpus_raw_shaw,\n",
    "               corpus_raw_wells, corpus_raw_wilde]\n",
    "\n",
    "#Calculate the length of the corpus\n",
    "len(corpus_raw_austen + corpus_raw_chesterton +\n",
    "                corpus_raw_conandoyle + corpus_raw_dickens +\n",
    "                corpus_raw_elliot + corpus_raw_huxley +\n",
    "                corpus_raw_shakespeare + corpus_raw_shaw +\n",
    "               corpus_raw_wells + corpus_raw_wilde)\n",
    "\n",
    "#Close the book file \n",
    "book_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleanse and parse and tokenize text###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before generating the features, and to increase the explanatory power of them, text has been cleaned and parsed accordingly.  The books have gone through an initial set of cleansing actions before been parsed using Spacy, to reduce the computing effort required by the latter and then have been cleaned again before the feature generation.\n",
    "\n",
    "The initial cleansing action has had three steps. The first step consisted on deleting all references to the Gutenberg Project from every book. This way, it has been avoided that words like “Gutenberg” and “Gutenberg Project” appear as features and distort the clustering of the authors.\n",
    "\n",
    "As described below, cleaning actions have gone from removing all references to chapters, digits double whitespaces and references to numbers like dates and ordinal numbers. This has been followed by removing punctuation and common stop words that will only add noise to the features that are generated afterwards.\n",
    "\n",
    "The remaining words, considered to have the most explanatory power regarding each of the titles from the authors, have been lemmatized and stemmed reducing up to 60% the computing resources needed. In the first case words from the same family are reduced to their lemmas and in the second case, additional prefixes and suffixes are removed. All cleaning operations have been carried out in a way that remaining sentences are stored in a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a set of stopwords in english from nltk\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "# Create a set of punctuation marks to exclude them from the text\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "#List of words that are in the books and can be removed from the text as they add noise\n",
    "comms = ['im','mr', 'miss', 'lady', 'sir', 'man', 'it', 'i', 'u', 'you', 'one'] \n",
    "\n",
    "#Call the lemmatizer and the stemmer\n",
    "lemma = WordNetLemmatizer()\n",
    "stemms = PorterStemmer()\n",
    "\n",
    "#Define a cleaning function that incorporates the different steps in the pipeline to clean the texts\n",
    "\n",
    "def clean(doc):\n",
    "    #Delete '--' from texts, specially used by Austen\n",
    "    doc = re.sub(r'--','',doc) \n",
    "    doc = re.sub(\"[\\[].*?[\\]]\", \"\", doc) #\n",
    "    #Remove references to chapters\n",
    "    doc = re.sub(r'Chapter \\d+', '', doc) \n",
    "    doc = re.sub(r'CHAPTER .*', '', doc)\n",
    "    #Substitute numbers by spaces\n",
    "    doc = re.sub('[0-9]+', '', doc)\n",
    "    #Remove digits and expressions such as 11th\n",
    "    doc = re.sub(\"\\s\\s+\", \" \", doc)\n",
    "    doc = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", doc)\n",
    "    #Remove quotes\n",
    "    doc = doc.replace('“', '').replace('”', '').replace('\"', '').replace('‘', '').replace('’', '')\n",
    "    #Remove stop words\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    #Remove punctuation\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    #Remove common words\n",
    "    commonalities_free = \" \".join([i for i in punc_free.lower().split() if i not in comms])\n",
    "    #Lemmatize and stem words to reduce the total amount of words\n",
    "    lemmatized = \" \".join(lemma.lemmatize(word) for word in commonalities_free.split())\n",
    "    normalized = \" \".join(stemms.stem(word) for word in lemmatized.split())\n",
    "    return normalized\n",
    "\n",
    "#Create a list of lists with all the documents\n",
    "doc_clean = [clean(doc) for doc in doc_complete]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the initial cleaning, novels are parsed using Spacy to tokenize and label the data and each of the set of words is identified by author. At this point, the integrity of the novels is lost and the dataset consists on tokens from different authors tagged to each of them. This will be useful for further operations and especially when using further supervised and unsupervised classification techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the cleaned novels\n",
    "#load spacy for english language as all novels are in english\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#Parse novels one by one to maintain the author tagging\n",
    "austen_doc = nlp(doc_clean[0])\n",
    "chesterton_doc = nlp(doc_clean[1])\n",
    "conandoyle_doc = nlp(doc_clean[2])\n",
    "dickens_doc = nlp(doc_clean[3])\n",
    "elliot_doc = nlp(doc_clean[4])\n",
    "huxley_doc = nlp(doc_clean[5])\n",
    "shakespeare_doc = nlp(doc_clean[6])\n",
    "shaw_doc = nlp(doc_clean[7])\n",
    "wells_doc = nlp(doc_clean[8])\n",
    "wilde_doc = nlp(doc_clean[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens are grouped into sentences and labelled in a dataframe. A Target column is added to the dataframe for further use when classifying texts using supervised techniques. Some additional cleansing of the sentences has been carried out. Sentences under 50 words have been removed from the dataset, duplicates and empty rows.Additionally the length of each sentence has been added to the daataframe.\n",
    "\n",
    "The graph below reflects the total number of characters and the contribution of each of the corpus to the total corpora. As titles were picked randomly, the contribution of each corpora is different by each author being the biggest Elliot and the smallest Huxley with nearly 8000 and 1500 contribution each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fa8f062fba8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA64AAAJCCAYAAADA95o/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xm4ZWdZJ+zfQxJExgQp8mEAAxpBoGUwzKgoEIYISSsoiBIhn9FPZPBDJDhFQNo4NCi0olFCB1QggkgEBCJzowwJhFk6kSkxNAkkDIICgaf/2OvAoajhnNQ+Z721676v61x7rXevferZ66ratX9rvUN1dwAAAGBUV5m7AAAAANgTwRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADC0g+cuYE+ud73r9ZFHHjl3GQAAAGyBc88995PdvWNvxw0dXI888sicc845c5cBAADAFqiqj27kOF2FAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChHTx3AXM58uSXz13Cpnzk1GPnLgEAAGAW7rgCAAAwNMEVAACAoQmuAAAADG2vwbWqblZV5637+WxVPbaqrltVZ1fV+dPjYdPxVVXPqKoLqurdVXW7db/rhOn486vqhK18YwAAAKyGvQbX7v5gd9+mu2+T5PuSfCHJS5KcnOQ13X1UktdM+0ly3yRHTT8nJXlWklTVdZOckuSOSe6Q5JS1sAsAAAC7s9muwvdI8q/d/dEkxyU5Y2o/I8nx0/ZxSZ7bC29JcmhV3SDJvZOc3d2XdfflSc5Ocp99fgcAAACstM0G1wcnef60fXh3fzxJpsfrT+1HJLlw3Wsumtp21w4AAAC7teHgWlVXTfKAJH+zt0N30dZ7aN/5zzmpqs6pqnMuvfTSjZYHAADAitrMHdf7JnlHd39i2v/E1AU40+MlU/tFSW607nU3THLxHtq/QXef1t1Hd/fRO3bs2ER5AAAArKLNBNeH5OvdhJPkrCRrMwOfkOSl69ofNs0ufKckn5m6Er8qyTFVddg0KdMxUxsAAADs1sEbOaiqrp7kXkl+bl3zqUnOrKoTk3wsyYOm9lckuV+SC7KYgfjhSdLdl1XVU5K8fTruyd192T6/AwAAAFbahoJrd38hybft1PapLGYZ3vnYTvLI3fye05OcvvkyAQAAOFBtdlZhAAAA2FaCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGgbCq5VdWhVvaiq/qWqPlBVd66q61bV2VV1/vR42HRsVdUzquqCqnp3Vd1u3e85YTr+/Ko6YaveFAAAAKtjo3dc/yjJK7v75kluneQDSU5O8pruPirJa6b9JLlvkqOmn5OSPCtJquq6SU5Jcsckd0hyylrYBQAAgN3Za3Ctqmsn+YEkz06S7v5Sd386yXFJzpgOOyPJ8dP2cUme2wtvSXJoVd0gyb2TnN3dl3X35UnOTnKfpb4bAAAAVs5G7rjeNMmlSZ5TVe+sqr+oqmskOby7P54k0+P1p+OPSHLhutdfNLXtrv0bVNVJVXVOVZ1z6aWXbvoNAQAAsFo2ElwPTnK7JM/q7tsm+Xy+3i14V2oXbb2H9m9s6D6tu4/u7qN37NixgfIAAABYZRsJrhcluai73zrtvyiLIPuJqQtwpsdL1h1/o3Wvv2GSi/fQDgAAALu11+Da3f8nyYVVdbOp6R5J3p/krCRrMwOfkOSl0/ZZSR42zS58pySfmboSvyrJMVV12DQp0zFTGwAAAOzWwRs87lFJ/qqqrprkQ0kenkXoPbOqTkzysSQPmo59RZL7JbkgyRemY9Pdl1XVU5K8fTruyd192VLeBQAAACtrQ8G1u89LcvQunrrHLo7tJI/cze85PcnpmykQAACAA9tG13EFAACAWQiuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGgHz10Aq+vIk18+dwmb9pFTj527BAAAYCfuuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaBsKrlX1kap6T1WdV1XnTG3Xraqzq+r86fGwqb2q6hlVdUFVvbuqbrfu95wwHX9+VZ2wNW8JAACAVbKZO64/1N236e6jp/2Tk7ymu49K8pppP0num+So6eekJM9KFkE3ySlJ7pjkDklOWQu7AAAAsDv70lX4uCRnTNtnJDl+Xftze+EtSQ6tqhskuXeSs7v7su6+PMnZSe6zD38+AAAAB4CNBtdO8uqqOreqTpraDu/ujyfJ9Hj9qf2IJBeue+1FU9vu2gEAAGC3Dt7gcXft7our6vpJzq6qf9nDsbWLtt5D+ze+eBGMT0qSG9/4xhssDwAAgFW1oTuu3X3x9HhJkpdkMUb1E1MX4EyPl0yHX5TkRutefsMkF++hfec/67TuPrq7j96xY8fm3g0AAAArZ6/BtaquUVXXWttOckyS9yY5K8nazMAnJHnptH1WkodNswvfKclnpq7Er0pyTFUdNk3KdMzUBgAAALu1ka7Chyd5SVWtHf/X3f3Kqnp7kjOr6sQkH0vyoOn4VyS5X5ILknwhycOTpLsvq6qnJHn7dNyTu/uypb0TAAAAVtJeg2t3fyjJrXfR/qkk99hFeyd55G5+1+lJTt98mQAAAByo9mU5HAAAANhygisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0A6euwDgyjvy5JfPXcKmfeTUY+cuAQCA/Yw7rgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAY2oaDa1UdVFXvrKqXTfs3qaq3VtX5VfXCqrrq1P4t0/4F0/NHrvsdT5zaP1hV9172mwEAAGD1bOaO62OSfGDd/u8meXp3H5Xk8iQnTu0nJrm8u78rydOn41JVt0jy4CS3THKfJH9SVQftW/kAAACsug0F16q6YZJjk/zFtF9JfjjJi6ZDzkhy/LR93LSf6fl7TMcfl+QF3f3F7v5wkguS3GEZbwIAAIDVtdE7rn+Y5FeSfHXa/7Ykn+7uK6b9i5IcMW0fkeTCJJme/8x0/Nfad/Gar6mqk6rqnKo659JLL93EWwEAAGAV7TW4VtWPJLmku89d37yLQ3svz+3pNV9v6D6tu4/u7qN37Nixt/IAAABYcQdv4Ji7JnlAVd0vydWSXDuLO7CHVtXB013VGya5eDr+oiQ3SnJRVR2c5DpJLlvXvmb9awAAAGCX9nrHtbuf2N037O4js5hc6bXd/dAkr0vywOmwE5K8dNo+a9rP9Pxru7un9gdPsw7fJMlRSd62tHcCAADAStrIHdfdeUKSF1TVbyd5Z5JnT+3PTvK8qrogizutD06S7n5fVZ2Z5P1JrkjyyO7+yj78+QBb7siTXz53CZv2kVOPnbsEAICl2lRw7e7XJ3n9tP2h7GJW4O7+zyQP2s3rn5rkqZstEgAAgAPXZtZxBQAAgG0nuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGNrBcxcAAEee/PK5S9iUj5x67NwlAMABxR1XAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0PYaXKvqalX1tqp6V1W9r6qeNLXfpKreWlXnV9ULq+qqU/u3TPsXTM8fue53PXFq/2BV3Xur3hQAAACrYyN3XL+Y5Ie7+9ZJbpPkPlV1pyS/m+Tp3X1UksuTnDgdf2KSy7v7u5I8fTouVXWLJA9Ocssk90nyJ1V10DLfDAAAAKtnr8G1F/592j1k+ukkP5zkRVP7GUmOn7aPm/YzPX+Pqqqp/QXd/cXu/nCSC5LcYSnvAgAAgJW1oTGuVXVQVZ2X5JIkZyf51ySf7u4rpkMuSnLEtH1EkguTZHr+M0m+bX37Ll4DAAAAu7Sh4NrdX+nu2yS5YRZ3Sb9nV4dNj7Wb53bX/g2q6qSqOqeqzrn00ks3Uh4AAAArbFOzCnf3p5O8PsmdkhxaVQdPT90wycXT9kVJbpQk0/PXSXLZ+vZdvGb9n3Fadx/d3Ufv2LFjM+UBAACwgjYyq/COqjp02v7WJPdM8oEkr0vywOmwE5K8dNo+a9rP9Pxru7un9gdPsw7fJMlRSd62rDcCAADAajp474fkBknOmGYAvkqSM7v7ZVX1/iQvqKrfTvLOJM+ejn92kudV1QVZ3Gl9cJJ09/uq6swk709yRZJHdvdXlvt2AAAAWDV7Da7d/e4kt91F+4eyi1mBu/s/kzxoN7/rqUmeuvkyAQAAOFBtaowrAAAAbDfBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAY2kbWcQUA9nNHnvzyuUvYtI+ceuzcJQAwCHdcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChCa4AAAAMTXAFAABgaIIrAAAAQxNcAQAAGJrgCgAAwNAEVwAAAIYmuAIAADA0wRUAAIChHTx3AQAAq+DIk18+dwmb9pFTj527BIANcccVAACAoQmuAAAADE1wBQAAYGiCKwAAAEPba3CtqhtV1euq6gNV9b6qeszUft2qOruqzp8eD5vaq6qeUVUXVNW7q+p2637XCdPx51fVCVv3tgAAAFgVG5lV+Iokj+vud1TVtZKcW1VnJ/mZJK/p7lOr6uQkJyd5QpL7Jjlq+rljkmcluWNVXTfJKUmOTtLT7zmruy9f9psCAGA17W+zN5u5GZZjr3dcu/vj3f2OaftzST6Q5IgkxyU5YzrsjCTHT9vHJXluL7wlyaFVdYMk905ydndfNoXVs5PcZ6nvBgAAgJWzqTGuVXVkktsmeWuSw7v748ki3Ca5/nTYEUkuXPeyi6a23bXv/GecVFXnVNU5l1566WbKAwAAYAVtOLhW1TWTvDjJY7v7s3s6dBdtvYf2b2zoPq27j+7uo3fs2LHR8gAAAFhRGwquVXVIFqH1r7r7b6fmT0xdgDM9XjK1X5TkRutefsMkF++hHQAAAHZrI7MKV5JnJ/lAdz9t3VNnJVmbGfiEJC9d1/6waXbhOyX5zNSV+FVJjqmqw6YZiI+Z2gAAAGC3NjKr8F2T/HSS91TVeVPbryY5NcmZVXViko8ledD03CuS3C/JBUm+kOThSdLdl1XVU5K8fTruyd192VLeBQAAACtrr8G1u/9Xdj0+NUnusYvjO8kjd/O7Tk9y+mYKBAAA4MC2qVmFAQAAYLsJrgAAAAxNcAUAAGBogisAAABDE1wBAAAY2kaWwwEAAA4QR5788rlL2LSPnHrs3CWwxdxxBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEMTXAEAABia4AoAAMDQBFcAAACGJrgCAAAwNMEVAACAoQmuAAAADE1wBQAAYGiCKwAAAEM7eO4CAAAADiRHnvzyuUvYtI+ceuysf747rgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMLS9BteqOr2qLqmq965ru25VnV1V50+Ph03tVVXPqKoLqurdVXW7da85YTr+/Ko6YWveDgAAAKtmI3dc/2eS++zUdnKS13T3UUleM+0nyX2THDX9nJTkWcki6CY5Jckdk9whySlrYRcAAAD2ZK/BtbvfmOSynZqPS3LGtH1GkuPXtT+3F96S5NCqukGSeyc5u7sv6+7Lk5ydbw7DAAAA8E2u7BjXw7v740kyPV5/aj8iyYXrjrtoattdOwAAAOzRsidnql209R7av/kXVJ1UVedU1TmXXnrpUosDAABg/3Nlg+snpi7AmR4vmdovSnKjdcfdMMnFe2j/Jt19Wncf3d1H79ix40qWBwAAwKq4ssH1rCRrMwOfkOSl69ofNs0ufKckn5m6Er8qyTFVddg0KdMxUxsAAADs0cF7O6Cqnp/k7kmuV1UXZTE78KlJzqyqE5N8LMmDpsNfkeR+SS5I8oUkD0+S7r6sqp6S5O3TcU/u7p0nfAIAAIBvstfg2t0P2c1T99jFsZ3kkbv5PacnOX1T1QEAAHDAW/bkTAAAALBUgisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBogisAAABDE1wBAAAYmuAKAADA0ARXAAAAhia4AgAAMDTBFQAAgKEJrgAAAAxNcAUAAGBo2x5cq+o+VfXBqrqgqk7e7j8fAACA/cu2BteqOijJHye5b5JbJHlIVd1iO2sAAABg/7Ldd1zvkOSC7v5Qd38pyQuSHLfNNQAAALAfqe7evj+s6oFJ7tPd/++0/9NJ7tjdv7jumJOSnDTt3izJB7etwOW4XpJPzl3EinOOt4fzvPWc463nHG8953h7OM9bzznees7x1tsfz/F3dPeOvR108HZUsk7tou0bknN3n5bktO0pZ/mq6pzuPnruOlaZc7w9nOet5xxvPed46znH28N53nrO8dZzjrfeKp/j7e4qfFGSG63bv2GSi7e5BgAAAPYj2x1c357kqKq6SVVdNcmDk5y1zTUAAACwH9nWrsLdfUVV/WKSVyU5KMnp3f2+7axhG+y33Zz3I87x9nCet55zvPWc463nHG8P53nrOcdbzzneeit7jrd1ciYAAADYrO3uKgwAAACbIrgCAAAwNMEVAACAoQmuAAAADE1wXYKqesxG2mB/UlVXqaprz13Hqqmqu1XVw6ftHVV1k7lrWjVV9YiqOmruOlZdVR1UVd9eVTde+5m7plVSVd9ZVd8ybd+9qh5dVYfOXdcqqYWfqqrfnPZvXFV3mLuuVVVV15i7hlVVVbeau4btILguxwm7aPuZ7S5iVVXVj1bV+VX1mar6bFV9rqo+O3ddq6iq/rqqrj395/L+JB+sqsfPXdeqqKpTkjwhyROnpkOS/OV8Fa2sI5P8WVX9a1WdWVWPqqrbzF3UKqmqRyX5RJKzk7x8+nnZrEWtnhcn+UpVfVeSZye5SZK/nreklfMnSe6c5CHT/ueS/PF85aymqrpLVb0/yQem/VtX1Z/MXNaq+dOqeltV/cIqX+CyHM4+qKqHJPnJJHdL8qZ1T10ryVe6+56zFLZiquqCJPfv7g/MXcuqq6rzuvs2VfXQJN+XRcg6t7u/d+bSVkJVnZfktkne0d23ndre7fxujar61iQ/m+SXkxzR3QfNXNLKmD6X79jdn5q7llVVVe/o7ttNFw//s7ufWVXvXPvsYN+tO8fvXPeZ/K7uvvXcta2SqnprkgcmOWvdeX5vdx8Qdwm3y9TT6BFJHpTkbUme091nz1vVch08dwH7uX9K8vEk10vy39e1fy7Ju2epaDV9QmjdNodU1SFJjk/yP7r7y1Xl6tbyfKm7e+2c6ja1Narq15PcNck1k7wzi+D6pj2+iM26MMln5i5ixX15ukB+QpL7T22HzFjPKvpyVR2UZO0zeUeSr85b0mrq7guran3TV+aqZVV19/nT/3/nJHlGktvW4qT/anf/7bzVLYfgug+6+6NJPprkzlV1eJLbT099oLuvmK+ylXNOVb0wyd8l+eJa46r8IxzMnyX5SJJ3JXljVX1HEt2yl+fMqvqzJIdW1c9mcWX0z2euaRX9aJIrsui++oYkb+nu/5y3pJXzoSSvr6qX5xs/l582X0kr5+FJfj7JU7v7w9N4eEMLlusZSV6S5PpV9dQs7gr++rwlraQLq+ouSbqqrprk0Zm6DbMcVfW9WXxmHJvFEI77d/c7qurbk/xzkpX4zqyr8BJU1YOS/EGS1yepJN+f5PHd/aI561oVVfWcXTR3dz9i24s5AFXVwS7ELE9V3SvJMVl8Vrxq1brxjKKqrpXFMI67JfnxLHpu3G3eqlbHNF77m3T3k7a7FtgXVXXzJPfI4jP5NXp4LV9VXS/JHyW5Zxbn+dVJHmOowfJU1RuzuBD+ou7+j52e++nuft48lS2X4LoEVfWuJPfq7kum/R1J/tEYCfY30wyWP5bF5DZf65HR3U+eqybYrGl2xe9P8oNJjs6iW+ubuvs3Zy1sBU0XCLq7/33uWlZFVb0nU9fVXTEmft9V1XX39Hx3X7Zdtay6qSv2o7v76XPXsuqmeR1u3N0fnLuWraKr8HJcZS20Tj4VMzYvTVV9d5JnJTm8u281dYd4QHf/9sylraKXZjFu7dys6/7Hvqmqz2XxRbTyjV9IK4sv/ZYdWq7fTfLGLLoBvr27vzxzPStnujjwvCTXnfY/meRh3f2+WQtbDT8ydwEHgHPz9c/kGye5fNo+NMnHspjBmSXo7q9U1XFJBNctVFX3z6L351WT3GSaSf/J3f2AeStbLndcl6Cqfj/J9yZ5/tT0E0ne3d1PmK+q1VFVb0jy+CR/Zja6reW8AhtRVf+U5Ne6+3XT/t2T/LfuvsushcEmVNWfZjHT7Sum/fsmuWd3P27eylbLNH74OklemOTza+3d/Y7ZiloxVXVukh9O8vpVXrXAHdcl6O7HV9WPZTGLZSU5rbtfMnNZq+Tq3f22nWajM+Zya/xTVf2X7n7P3IWsoqr6gySnd/f7565llU1LAvxOklskudpae3ffdLaiVs811kJrknT3682SvRzremgki+8UybrRMFLIAAAScklEQVQeG3poLNXtu/vn13a6+x+q6ilzFrSi1i5orR921FkELZbjiu7+zE7flVeO4Lok3f3iLBYLZ/k+WVXfma9PV//ALJYhYvnuluRnqurDWXQVXvuitFJX7Gb0L0n+vKoOTvKcJM/vbkuKLN9zkpySRde0H8pipsXV/t98+32oqn4ji+7CSfJTST48Yz0ro7uvNXcNB5BPTsuH/GUW3zF+KovhXixRd//Q3DUcAN5bVT+Z5KDp4u2js1i2c6XoKrwPqup/dffddro6mrgqulRVddMkp2Vxxe7yLL4cPXRajoglmpa/+SbO9XJV1c2yCFMPSfLmJH++/u4V+6aqzu3u76uq93T3f5na3tTd3z93bauiqg5L8qQsLnYlizHFv9Xdn56vqtVTVXdLclR3P2eamfVa3e0CwZJMkzSdkuQHpqY3JnmSyZmWr6qOTXLLfGMvGBM/LklVXT3Jr2XdqgVJnrJqS8EJrgyvqm4yrWF3jSwmwvrcWtvcta2inb4o7UhyTed6eaYZFn8ki+B6oyRnZvHl//Pd/eA5a1sVVfXmLGYVflGS1yb5tySndvfNZi1shVTVg7r7b/bWxpU3LTl0dJKbdfd3T+sx/k1333Xm0mBTprHEV8+iB8xfZLFe7tu6+8RZC2O/I7juA9Opb4+qekd3326ntnO7+/vmqmlV+aK0tarqaUkekOQ1SZ7d3W9b99wHBavlqKrbZ7G4/aFJnpLFpCC/191vmbWwFbKbz+VvauPKq6rzktw2yTtWebKVOVTV32fPSw6t1Eysc1v7e7vu8ZpJ/ra7j5m7tv3dgfZ32RjXfbN+OvWddRITgeyDaVHwWya5TlX96Lqnrp11XU1Yqv+a6YtSknT3xdM6jSzHe5P8end/YRfP3WG7i1lV3f32afPfs7izzZJMs67eL8kRVfWMdU9dOybNW7YvdXdX1dr8Dia/Wp4/mLuAA8x/TI9fmC6IfyqWHFqWA+rvsuC6D7rbP7qtdbMsulQemuT+69o/l+RnZ6lo9fmitIW6+/SqekBVrY2nekN3//30nEmalmRa+/nxSb4j6/6f624zWO67i5Ock0XPgXPXtX8uyS/NUtHqOrOq/izJoVX1s0kekeTPZ65pJXT3G+au4QDzsqo6NMnvZ3FhvLPoMsw+Wvu7XFXX7+5L1j83zaexUnQV3gdVtccuUdanWo6qunN3//PcdRwIquqXkxyV5F5ZLCfyiCxmvn3GHl/IhlTV72RxZ/WvpqaHJDmnu584X1Wrp6releRPswhWX1lr7+5zd/siNmwap/3c7n7o3LWsoqp6bBaTtr0zizGBX5tspbvPnrO2VVFVZ3b3j1fVe7LryTV1x94iVfUtSa7mYu1yVdUHk/xGd5857T8uyYndfYt5K1suwXUfVNWeZgFtV/eXo6p+L8lvZ9HV5JVJbp3ksd39l7MWtqKq6l7xRWlLVNW7k9ymu7867R+U5J2+JC2XMfBbr6pemeQB3f2luWtZNdN6z3dJcvMk785iSYs3J/lnc2csR1U9Mck/ZrFSwZd3ft5M+ss1zXj7uCQ37u6fnZZruVl3v2zm0lZGVd0gixU4/jPJ4VnM8/C47v73WQtbMsGV4VXVed19m6r6r0mOz6I72uu6+9Yzl7Zyquq+3f0PO7X9fHf/6Vw1rZIpuN597cvnNMHb6wXX5Vg3Yd6jk1yS5CVZrEecxIR5yzR1Yb1dkrOSfH6tvbufNltRK6aqrprFZHl3SXLn6efTq3YHZQ4uDmyvqnphFj1gHtbdt6qqb83iXN9m5tJWSlU9MskTk3w1yUO6+80zl7R0xrjug6r6le7+vWn7G5YBqKr/1t2/Ol91K+WQ6fF+WXRbvaxqV/NhsQS/UVVf7O7XJklVPSHJ3bPodsm++50k75x6a1QWawfqJrw8O0+Y98s7PW/CvOW5ePq5ShITuG2Nb81i0qvrTD8XJ3nPrBWtiO7+5eSbLg48IsmfV5WLA8v3nd39E1X1kCTp7v8oX+SWqqrOTvLxJLdKcsMkp1fVG9f+rq8KwXXfPDjJ703bT0yyfv26+yQRXJfj76vqX7LoKvwL09qiK7Wg8kAekMUkCo/P4u/wzac2lqC7n19Vr09y+yzC1RO6+//MW9VK+YkkF3b3x5Okqk5I8mNJPpLkt+Yra/V095OSxQRu3f35vR3PxlXVaVnMqP+5JG/N4m7g07r78lkLW00uDmyPL013WdcmfvzOrOsNw1L8cXf/3bT96aq6S1bwwriuwvugqt65bm21r23vap99U1WHJflsd39lGitxbV/4t0ZVXT+LsT/nJnlE+5DYZyZy2x5V9Y4k95x6ZfxAkhckeVSS2yT5nu5+4KwFrpCqunOSZye5ZnffuKpuneTnuvsXZi5tvzeNH75eFstn/VOSf07yXp/Fy7OLiwNvSfIWFwe2xjR3xq8nuUWSVye5a5Kf6e7Xz1kX+x93XPdN72Z7V/tcSVX1sHXb65967vZXs5qq6nP5ehfLTnLVLLpVPrCquruvPWd9K+C/T49Xy6Jb2ruyONffm8WXprvNVNeqOWjd+LSfSHJad784yYur6rwZ61pFf5jk3lmMcU13v2vdMk/sg+6+z9SN8pZZdGF9XJJbVdVlWYwLPGXWAlfDjZN8S5Lzk/xbkouSfHrWilZYd589XVi8Uxb/9z2muz85c1krZd33uGTxHe6QJP/e3deZr6rlE1z3za2r6rNZ/CP81mk70/7V5itr5dx+3fbVktwji3XABNcl6W5j1LZQd/9QklTVC5Kc1N3vmfZvlW8eh8mVd1BVHdzdV2TxOXHSuuf8f7dk3X3hThcTv7K7Y9mc6e7qe6vq00k+M/38SBbLaQmu+8jFgVn8YBYXaTuLUPWSectZLTt/j6uq47P4vFgp/iPfB9190Nw1HAi6+1Hr96vqOkmeN1M5K22aufm1a+urTQuG333duAn2zc3XQmuSdPd7q8qsisvz/CRvqKpPZjEm/k1JUlXflcUXf5bnwmkMVU8T3Dw6i+UX2EdV9egswtRds1iq5c1ZdBc+PcZfLo2LA9unqv4kyXdl8RmdJD9XVffs7kfOWNZK6+6/q6qT565j2YxxZb9TVYckeU9333zuWlbN2tJDO7UZr70kVfX8LJYO+cssrjr/VBZjBB8ya2ErpKrulOQGSV69NmlQVX13FufZWOIlqarrJfmjJPfMopfRq5M82lIi+66qnpZpeZa1icZYrj1cHHhzFt8vvjpjeSunqt6X5FZr47Sr6ipZnOdbzlvZ6qiqH123e5UshiX9YHffeaaStoQ7rgyvqv4+X++3f1CS70ly5nwVrbSr7KLN58TyPDzJ/5fkMdP+G5M8a75yVk93v2UXbf97jlpW3M26+6HrG6rqrll88WcfdPf/P3cNB4Ajk7woyS+5OLAtPpjFuOKPTvs3ymL9XJbn/uu2r8hiNv3j5ill67jjyvCq6gfX7V6RxdX9h+hisnxVdXoWE1T8cRYXCx6V5LDu/pk56wLGUlXv6O7b7a0NoKrekMV8JW+bmm6fxR3uLyRJd1t2jw1xJ4XhdfcbpnGAP5nkx5N8OMmL561qZT0qyW8keWG+3v3PBYIlme5I/VaS78i6z9/uvulcNcFmTMvg3CXJjqpaf2fw2ln0iAHY2W/OXcCqqqpnZg8rmXT3o7exnC0nuDKsaVzag5M8JMmnMoWptRlaWb5pTODKDeYfyLOT/FIWa+SagZX90VWTXDOL7w/rZ7H8bBLr5AK7ck6S/+jur07f7W6e5B+6+8sz17UKzlm3/aSs+MRiugozrKr6ahazgp7Y3RdMbR9yd2r5quoPu/uxO40n/hrdeJajqt7a3Xecuw7YV1X1Hd390Wn7KllMfvXZvbwMOABV1blJvj/JYUnekkXY+sLO4+TZNwfCZJruuDKyH8vijuvrquqVSV6QRfdVlm9teaE/mLWK1fe6qvr9JH+b5ItrjWa7ZT/0O1X181n0HDg3yXWq6mnd/fsz1wWMp7r7C1V1YpJndvfvVdV5cxe1glb+bqTgyrC6+yVJXlJV10hyfBZdLA+vqmcleUl3v3rWAldId587Pb6hqnZM25fOW9VKWrvbevS6tk7ywzPUAvviFt392ap6aJJXJHlCFgFWcAV2VtP4+IcmOXFqMyaeTRNcGd407vKvkvxVVV03yYOyGIcpuC5JVVUW4yJ+MYu72lepqiuyuDL65FmLWyHGZ7NCDpnW1D4+yf/o7i9X1cpf7QeulMcmeWIWNx3eV1U3TfK6mWtaCVX1uXz9TuvVq2ptyEYl6e6+9jyVbQ1jXIFU1S8luV+Sk7r7w1PbTbNYY/SV3f30OetbJVV1bJJbJrnaWpuLA+xvqurRWdxlfVeSY7NYo/Evu/v7Zy0MGFZVXWO6GQFXiuAKpKremeRe3f3Jndp3JHn1qg/23y5V9adJrp7kh5L8RRazsL6tu0/c4wthP1BVB3f3FXPXAYxl6ib87CwmcbtxVd06yc919y/MXBr7mavMXQAwhEN2Dq3J18a5HjJDPavqLt39sCSXd/eTktw5yY1mrgk2raoOr6pnV9U/TPu3SHLCzGUBY/rDJPfOYmnDdPe7kvzArBWxXxJcgST50pV8js35j+nxC1X17Um+nOQmM9YDV9b/TPKqJN8+7f/vLMaxAXyT7r5wpyZrmbNpJmcCkuTW6wb0r1dZNxaTffayqjo0i5lX35HFhAp/MW9JcKVcr7vPrKonJkl3X1FVvogCu3JhVd0lSVfVVZM8OskHZq6J/ZDgCqS7TUu/Dbr7KdPmi6vqZUmu1t2fmbMmuJI+X1Xflmk2y6q6UxJ/l4Fd+fkkf5TkiCQXZbEqxCNnrYj9ksmZALbRdNX5yKy7cNjdz52tILgSqup2SZ6Z5FZJ3ptkR5IHdve7Zy0MGE5VXbe7L9up7SZrqxjARgmuANukqp6X5DuTnJevj+/p7n70fFXBlVNVBye5WRZDCj7Y3V+euSRgQFX15iT37e7PTvvfk+RvuvtW81bG/kZwBdgmVfWBJLdoH7ysAL0HgI2Y1i//lSzWfL5ZkucmeWh3nzdrYex3jHEF2D7vTfL/JPn43IXAvthd74EsvpACfE13v7yqDslibOu1khzf3efPXBb7IcEVYPtcL8n7q+ptSb641tjdD5ivJLhSjo7eA8AeVNUzM03gNrl2kg8leVRVxTAZNktwBdg+vzV3AbAkeg8Ae3POTvvnzlIFK8MYV4BtVFWHJ7n9tPu27r5kznpgM6rq77O4g3KtJLdJovcAANvCHVeAbVJVP57k95O8PouZWJ9ZVY/v7hfNWhhs3FlJDk/ypp3afzDJv21/OcDoquqoJL+T5BZJrrbW3t03na0o9kuCK8D2+bUkt1+7y1pVO5L8YxLBlf3FcUn+b3t3DCLFGYZx/P+cICpWFoKVIRxWJhYeQQgIl9QpxMJKRMQuSBqbNF5nmTKXC0GsrjpsRFQwECwslJCQpDJgQooEUSGFmIjyppg9slkugdHdnZ3s/1ftfF/zFMMs73zzft/Ho+e1JnkKXAC+6CSVpFl2ieb58AmwDJymeXkrtbLQdQBJmiMLI58GP8bnsPrljdGiFaCq7tEcjSNJo3ZW1S2aFsWfq2oFeK/jTOohV1wlaXquJ7kBrA+uTwDXOswjtbXjP+Z2Ti2FpD75I8kCcD/JhzRtBXs7zqQe8k2/JE1YksUk71bVeeAz4G3gEHAHWOs0nNTO3SRnRweTnMEdQyVt7SNgF3AOOAycBE51mki95K7CkjRhSa6ydV/gEnChqj7oJpnUzmBX7CvAc/4uVJeA7cCxqvqtq2ySpP83C1dJmrAk31fVwX+Z+66q3pp2Jul1JFkGNu/pH6rqyy7zSJpdSQ4A54H9DLUpVpV9rmrFwlWSJizJj1W12HZOkqS+S/ItsErzlcbLzfGqsr1Arbg5kyRN3t0kZ6vq8+FB+wIlSXPgRVV92nUI9Z8rrpI0YfYFSpLmTZI9g5/ngIc0/4N/bs5X1ZMucqm/LFwlaUrsC5QkzYskD4ACMhj6R9FRVW9OPZR6zcJVkiRJ0lgleQf4pap+HVyfAo4DPwErrriqLc9xlSRJkjRuqww+DU5yFLgIXAZ+xzPM9QrcnEmSJEnSuG0bWlU9AaxV1QawkeSbDnOpp1xxlSRJkjRu25JsLpK9Dwzv6+DimVrzppEkSZI0buvAV0keAc+A2wBJFmk+F5ZacXMmSZIkSWOX5AiwD7hZVU8HYweA3VX1dafh1DsWrpIkSZKkmWaPqyRJkiRpplm4SpIkSZJmmoWrJEmSJGmmWbhKkiRJkmbaXyva5rJDaEPvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa8f062fa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group into sentences.\n",
    "austen_sents = [[str(sent), \"Austen\"] for sent in austen_doc.sents]\n",
    "chesterton_sents = [[str(sent), \"Chesterton\"] for sent in chesterton_doc.sents]\n",
    "conandoyle_sents = [[str(sent), \"Conandoyle\"] for sent in conandoyle_doc.sents]\n",
    "dickens_sents = [[str(sent), \"Dickens\"] for sent in dickens_doc.sents]\n",
    "elliot_sents = [[str(sent), \"Elliot\"] for sent in elliot_doc.sents]\n",
    "huxley_sents = [[str(sent), \"Huxley\"] for sent in huxley_doc.sents]\n",
    "shakespeare_sents = [[str(sent), 'Shakespeare'] for sent in shakespeare_doc.sents]\n",
    "shaw_sents = [[str(sent), \"Shaw\"] for sent in shaw_doc.sents]\n",
    "wells_sents = [[str(sent), \"Wells\"] for sent in wells_doc.sents]\n",
    "wilde_sents = [[str(sent), \"Wilde\"] for sent in wilde_doc.sents]\n",
    "\n",
    "#Combine the sentences from the two novels into one data frame.\n",
    "names = ['Sentences','Author']\n",
    "sentences = pd.DataFrame(austen_sents + chesterton_sents + conandoyle_sents +\n",
    "                         dickens_sents + elliot_sents + huxley_sents + \n",
    "                         shakespeare_sents + shaw_sents + wells_sents +\n",
    "                         wilde_sents, columns = names)\n",
    "\n",
    "#Aadd numerical column to tag the authors for supervised classification\n",
    "sentences.loc[sentences['Author'] == 'Austen', 'Target'] = 0\n",
    "sentences.loc[sentences['Author'] == 'Chesterton', 'Target'] = 1\n",
    "sentences.loc[sentences['Author'] == 'Conandoyle', 'Target'] = 2\n",
    "sentences.loc[sentences['Author'] == 'Dickens', 'Target'] = 3\n",
    "sentences.loc[sentences['Author'] == 'Elliot', 'Target'] = 4\n",
    "sentences.loc[sentences['Author'] == 'Huxley', 'Target'] = 5\n",
    "sentences.loc[sentences['Author'] == 'Shakespeare', 'Target'] = 6\n",
    "sentences.loc[sentences['Author'] == 'Shaw', 'Target'] = 7\n",
    "sentences.loc[sentences['Author'] == 'Wells', 'Target'] = 8\n",
    "sentences.loc[sentences['Author'] == 'Wilde', 'Target'] = 9\n",
    "\n",
    "#Add length of sentences to the Dataframe\n",
    "sentences['len'] = sentences['Sentences'].map(len)\n",
    "\n",
    "#Remove duplicate description columns to avoid having the initial and final information about Gutenberg project\n",
    "sentences = sentences.drop_duplicates('Sentences')\n",
    "\n",
    "#Remove rows that are empty during the cleaning process\n",
    "sentences = sentences[~sentences['Sentences'].isnull()]\n",
    "\n",
    "#remove sentences with length smaller than 50 characters to reduce the noise in the data\n",
    "sentences = sentences[sentences.len > 50]\n",
    "sentences.reset_index(inplace=True)\n",
    "sentences.drop('index', inplace=True, axis=1)\n",
    "\n",
    "#Plot the contribution of each author to the corpus (sentences)\n",
    "sentences.Author.value_counts().plot(kind='bar', grid=False, figsize=(16, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is split in a training and testing set with 25% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data in a train and test set\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(sentences['Sentences'], sentences['Target'],test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words are tokenized and an integer is assigned to them. Once they are converted into integers each book is represented by a matrix and the labels are represented by 10 binary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer(num_words=25000)\n",
    "tokenizer.fit_on_texts(X_tr)\n",
    "x_train = tokenizer.texts_to_matrix(X_tr, mode = \"tfidf\")\n",
    "x_test = tokenizer.texts_to_matrix(X_te, mode = \"tfidf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices\n",
    "# So instead of one column with 10 values, create 10 binary columns\n",
    "y_train = keras.utils.to_categorical(y_tr, 10)\n",
    "y_test = keras.utils.to_categorical(y_te, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi Layer Perceptron ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is set up and the parameters are tunned so that the model gives the highest accuracy considering the small dataset that we have for this exercise. In this case, and after several trials, the number of words is set up to 25000 being the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_61 (Dense)             (None, 32)                800032    \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 801,418\n",
      "Trainable params: 801,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Start the model\n",
    "model = Sequential()\n",
    "\n",
    "#Set up the outter layer and reduce the number of words in voc to the optimum 25000\n",
    "model.add(Dense(32, activation='relu', input_shape=(25000,)))\n",
    "\n",
    "# Dropout layers remove features and fight overfitting\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# End with a number of units equal to the number of classes we have for our outcome\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "#Show the model features\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='categorical_crossentropy',optimizer=RMSprop(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is evaluated on the train and test set achieving an overall accuracy of 86%. in this case, a two layers of 32 neurons each is considered with dropout to reduce overfitting and softmax is used as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25671 samples, validate on 8557 samples\n",
      "Epoch 1/5\n",
      "25671/25671 [==============================] - 13s 499us/step - loss: 1.0288 - acc: 0.7111 - val_loss: 0.4965 - val_acc: 0.8620\n",
      "Epoch 2/5\n",
      "25671/25671 [==============================] - 9s 342us/step - loss: 0.3385 - acc: 0.9017 - val_loss: 0.4159 - val_acc: 0.8702\n",
      "Epoch 3/5\n",
      "25671/25671 [==============================] - 8s 321us/step - loss: 0.1845 - acc: 0.9459 - val_loss: 0.4299 - val_acc: 0.8718\n",
      "Epoch 4/5\n",
      "25671/25671 [==============================] - 8s 324us/step - loss: 0.1177 - acc: 0.9652 - val_loss: 0.4748 - val_acc: 0.8704\n",
      "Epoch 5/5\n",
      "25671/25671 [==============================] - 8s 316us/step - loss: 0.0742 - acc: 0.9795 - val_loss: 0.5311 - val_acc: 0.8677\n",
      "Test loss: 0.531117866101\n",
      "Test accuracy: 0.867710646268\n"
     ]
    }
   ],
   "source": [
    "#Run the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "#Evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "#Print results\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set up the parameters for the LTSM models***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the LTSM and be able to do the dimensionality reduction the number of top words is defined setting the rest  to zero, the max review legnth truncating the senteces of each author and the embedding vector. After several trials the optimum parameters are the ones shown bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (25671, 500)\n",
      "25671 train samples\n",
      "8557 test samples\n"
     ]
    }
   ],
   "source": [
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train_CNN = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "X_test_CNN = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "\n",
    "# Vector length\n",
    "embedding_vector_length = 32\n",
    "\n",
    "#Print the shape of the train and test set\n",
    "print('x_train shape:', X_train_CNN.shape)\n",
    "print(X_train_CNN.shape[0], 'train samples')\n",
    "print(X_test_CNN.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Simple LTSM ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To baseline the results obtained using LTSM, a simple LTSM is run setting the number of features to 100 and having the ouput layer with 10 outputs as the number of authors used in the classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 214,210\n",
      "Trainable params: 214,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Start the model with no dropouts to fight overfitting\n",
    "model_LTSM = Sequential()\n",
    "model_LTSM.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "model_LTSM.add(LSTM(100))\n",
    "model_LTSM.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "#Show model features\n",
    "model_LTSM.summary()\n",
    "\n",
    "#Compile the model\n",
    "model_LTSM.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is evaluated, an accuracy of 90% is achieved in three epochs. This overall accuracy seems to be the maximum accuracy that can be obtained with the dataset available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25671 samples, validate on 8557 samples\n",
      "Epoch 1/3\n",
      "25671/25671 [==============================] - 228s 9ms/step - loss: 0.6010 - acc: 0.8936 - val_loss: 0.4956 - val_acc: 0.9000\n",
      "Epoch 2/3\n",
      "25671/25671 [==============================] - 224s 9ms/step - loss: 0.3953 - acc: 0.9000 - val_loss: 0.3372 - val_acc: 0.9000\n",
      "Epoch 3/3\n",
      "25671/25671 [==============================] - 226s 9ms/step - loss: 0.3234 - acc: 0.9000 - val_loss: 0.3166 - val_acc: 0.9000\n",
      "Test loss: 0.313094868262\n",
      "Test accuracy: 0.899999976158\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "model_LTSM.fit(X_train_CNN, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=3,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_CNN, y_test))\n",
    "\n",
    "#Evaluate the model\n",
    "score = model_LTSM_wdrop.evaluate(X_test_CNN, y_test, verbose=0)\n",
    "\n",
    "#Print scores\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LTSM with Dropout ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To the previous model, dropout is included to reduce overfitting. In this case, two forward dropouts are included with 0.2 dropout rate and a layer of 100 and 10 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_58 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 214,210\n",
      "Trainable params: 214,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Set up the model including dropouts to fight overfitting\n",
    "model_LTSM_wdrop = Sequential()\n",
    "model_LTSM_wdrop.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "model_LTSM_wdrop.add(Dropout(0.2))\n",
    "model_LTSM_wdrop.add(LSTM(100))\n",
    "model_LTSM_wdrop.add(Dropout(0.2))\n",
    "model_LTSM_wdrop.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "#Model Features\n",
    "model_LTSM_wdrop.summary()\n",
    "\n",
    "#Compile the model\n",
    "model_LTSM_wdrop.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum accuracy achieved continues to be 0.9 which is reasonable for the size of the dataset used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25671 samples, validate on 8557 samples\n",
      "Epoch 1/3\n",
      "25671/25671 [==============================] - 222s 9ms/step - loss: 0.3289 - acc: 0.8980 - val_loss: 0.3129 - val_acc: 0.9000\n",
      "Epoch 2/3\n",
      "25671/25671 [==============================] - 218s 8ms/step - loss: 0.3148 - acc: 0.9000 - val_loss: 0.3124 - val_acc: 0.9000\n",
      "Epoch 3/3\n",
      "25671/25671 [==============================] - 219s 9ms/step - loss: 0.3139 - acc: 0.9000 - val_loss: 0.3131 - val_acc: 0.9000\n",
      "Test loss: 0.313094868262\n",
      "Test accuracy: 0.899999976158\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "model_LTSM_wdrop.fit(X_train_CNN, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=3,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_CNN, y_test))\n",
    "\n",
    "#Evaluate the model\n",
    "score = model_LTSM_wdrop.evaluate(X_test_CNN, y_test, verbose=0)\n",
    "\n",
    "#Print the scores\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. LTSM & Convolutional Neural Network ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolutional neural network with LTSM is run and in this case thwew are three layers used. The layers have 32, 128 and 10 each maintaining the LTSM model with 100 parameters. The convolutional link in one dimension linking 32 neurons and using a kernel size 3 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 500, 32)           3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 250, 32)           0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 250, 128)          4224      \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 100)               91600     \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 259,938\n",
      "Trainable params: 259,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "# First convolutional layer, note the specification of shape\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(Conv1D(32, 3, padding='same',activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "#Model features\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieves thebest result possible 90%. Although low for a neural network is reasonable neough for the size of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25671 samples, validate on 8557 samples\n",
      "Epoch 1/3\n",
      "25671/25671 [==============================] - 226s 9ms/step - loss: 0.3277 - acc: 0.8978 - val_loss: 0.3128 - val_acc: 0.9000\n",
      "Epoch 2/3\n",
      "25671/25671 [==============================] - 225s 9ms/step - loss: 0.3149 - acc: 0.9000 - val_loss: 0.3127 - val_acc: 0.9000\n",
      "Epoch 3/3\n",
      "25671/25671 [==============================] - 223s 9ms/step - loss: 0.3142 - acc: 0.9000 - val_loss: 0.3126 - val_acc: 0.9000\n",
      "Test loss: 0.31263894511\n",
      "Test accuracy: 0.899999976158\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "model.fit(X_train_CNN, y_train,\n",
    "          batch_size=64,\n",
    "          epochs=3,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_CNN, y_test))\n",
    "\n",
    "#Evaluate the model\n",
    "score = model.evaluate(X_test_CNN, y_test, verbose=0)\n",
    "\n",
    "#Print the score\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. LTSM & Recurrent Neural Network ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the LTSM model is run with a recurrent neural network. This network has a recurrent dropout of 0.2 and three layers maintaining LTSM to 100 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 214,210\n",
      "Trainable params: 214,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vector_length, input_length=max_review_length))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "#Model features\n",
    "model.summary()\n",
    "\n",
    "# Compile the model to put it all together.\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is fit and the maximum result is achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25671 samples, validate on 8557 samples\n",
      "Epoch 1/3\n",
      "25671/25671 [==============================] - 170s 7ms/step - loss: 0.6543 - acc: 0.8582 - val_loss: 0.6123 - val_acc: 0.9000\n",
      "Epoch 2/3\n",
      "25671/25671 [==============================] - 170s 7ms/step - loss: 0.5663 - acc: 0.9000 - val_loss: 0.5139 - val_acc: 0.9000\n",
      "Epoch 3/3\n",
      "25671/25671 [==============================] - 170s 7ms/step - loss: 0.4549 - acc: 0.9000 - val_loss: 0.3976 - val_acc: 0.9000\n",
      "Test loss: 0.397608311492\n",
      "Test accuracy: 0.899999976158\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "model.fit(X_train_CNN, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=3,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test_CNN, y_test))\n",
    "\n",
    "#Evaluate the model\n",
    "score = model.evaluate(X_test_CNN, y_test, verbose=0)\n",
    "\n",
    "#Print the score\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Conclusion ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five different implementations of a neural network have been set up, tuned and run. When using LSTM and due to the size of the dataset the model achieves the highest result possible of 90% accuracy. Different parameters have been tunned in a trial and error way, from the to words to the size of the vectors until the maximum overall accuracy has been reached. In this case although the use of more comple models increases computing time, it does not increase the overall accuracy. Dropouts in the models help to reduce overfitting but due to the size of the dataset for these implementations the results are neglible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
